{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax_metrics as jm\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from functools import partial\n",
    "from jax import random\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Switch off the cache \n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = 'false'\n",
    "os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSELECT @@SERVERNAME\\n'"
      ]
     },
     "execution_count": 567,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyodbc \n",
    "\n",
    "query = \"\"\"\n",
    "\n",
    "SELECT @@SERVERNAME\n",
    "\"\"\"\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   id              48842 non-null  int64 \n",
      " 1   age             48842 non-null  int64 \n",
      " 2   workclass       46043 non-null  object\n",
      " 3   fnlwgt          48842 non-null  int64 \n",
      " 4   education       48842 non-null  object\n",
      " 5   education-num   48842 non-null  int64 \n",
      " 6   marital-status  48842 non-null  object\n",
      " 7   occupation      46033 non-null  object\n",
      " 8   relationship    48842 non-null  object\n",
      " 9   race            48842 non-null  object\n",
      " 10  sex             48842 non-null  object\n",
      " 11  capital-gain    48842 non-null  int64 \n",
      " 12  capital-loss    48842 non-null  int64 \n",
      " 13  hours-per-week  48842 non-null  int64 \n",
      " 14  native-country  47985 non-null  object\n",
      " 15  class           48842 non-null  object\n",
      "dtypes: int64(7), object(9)\n",
      "memory usage: 6.0+ MB\n"
     ]
    }
   ],
   "source": [
    "import mariadb\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "data = pd.read_csv ('csv_result-phpMawTba 2.csv',na_values='?')   \n",
    "df = pd.DataFrame(data)\n",
    "df.info()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a realizar una limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  age         workclass  fnlwgt     education  education-num  \\\n",
      "0   1   25           Private  226802          11th              7   \n",
      "1   2   38           Private   89814       HS-grad              9   \n",
      "2   3   28         Local-gov  336951    Assoc-acdm             12   \n",
      "3   4   44           Private  160323  Some-college             10   \n",
      "4   5   18               NaN  103497  Some-college             10   \n",
      "5   6   34           Private  198693          10th              6   \n",
      "6   7   29               NaN  227026       HS-grad              9   \n",
      "7   8   63  Self-emp-not-inc  104626   Prof-school             15   \n",
      "8   9   24           Private  369667  Some-college             10   \n",
      "9  10   55           Private  104996       7th-8th              4   \n",
      "\n",
      "       marital-status         occupation   relationship   race     sex  \\\n",
      "0       Never-married  Machine-op-inspct      Own-child  Black    Male   \n",
      "1  Married-civ-spouse    Farming-fishing        Husband  White    Male   \n",
      "2  Married-civ-spouse    Protective-serv        Husband  White    Male   \n",
      "3  Married-civ-spouse  Machine-op-inspct        Husband  Black    Male   \n",
      "4       Never-married                NaN      Own-child  White  Female   \n",
      "5       Never-married      Other-service  Not-in-family  White    Male   \n",
      "6       Never-married                NaN      Unmarried  Black    Male   \n",
      "7  Married-civ-spouse     Prof-specialty        Husband  White    Male   \n",
      "8       Never-married      Other-service      Unmarried  White  Female   \n",
      "9  Married-civ-spouse       Craft-repair        Husband  White    Male   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week native-country  class  \n",
      "0             0             0              40  United-States  <=50K  \n",
      "1             0             0              50  United-States  <=50K  \n",
      "2             0             0              40  United-States   >50K  \n",
      "3          7688             0              40  United-States   >50K  \n",
      "4             0             0              30  United-States  <=50K  \n",
      "5             0             0              30  United-States  <=50K  \n",
      "6             0             0              40  United-States  <=50K  \n",
      "7          3103             0              32  United-States   >50K  \n",
      "8             0             0              40  United-States  <=50K  \n",
      "9             0             0              10  United-States  <=50K  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\52333\\AppData\\Local\\Temp\\ipykernel_33712\\1370505908.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_numerical[j] = df_numerical[j] /df_numerical[j].abs().max()\n"
     ]
    }
   ],
   "source": [
    "print(df.head(10))\n",
    "#del anterior print, vemos que los valores faltantes los marca con signo '?'\n",
    "#quitamos los renglones con signo '?:\n",
    "df=df.dropna()\n",
    "\n",
    "\n",
    "\n",
    "#separamos las variables categóricas para después hacerlas one-hot encoding\n",
    "df_categorical=df[['workclass','education','marital-status','occupation','relationship','race','sex','native-country']]\n",
    "\n",
    "#separamos las columnas con variables con numero\n",
    "#al revisar el significado de cada columna, observamos que el significado de flnlwt no es claro, además contiene mas de 28 mil clases distintas\n",
    "#entonces no usamos la columna fnlwgt\n",
    "df_numerical=df[['age','education-num','capital-gain','capital-loss','hours-per-week']]\n",
    "\n",
    "columnas=[\"age\",\"education-num\",\"capital-gain\",\"capital-loss\",\"hours-per-week\"]\n",
    "for j in columnas:\n",
    "    df_numerical[j] = df_numerical[j] /df_numerical[j].abs().max()\n",
    "\n",
    "\n",
    "df.loc[df[\"class\"] == \"<=50K\", \"class\"] = 1\n",
    "df.loc[df[\"class\"] == \">50K\", \"class\"] = -1\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer un one hot encoder solo usando pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workclass\n",
      "Private\n",
      "Local-gov\n",
      "Self-emp-not-inc\n",
      "Federal-gov\n",
      "State-gov\n",
      "Self-emp-inc\n",
      "Without-pay\n",
      "education\n",
      "11th\n",
      "HS-grad\n",
      "Assoc-acdm\n",
      "Some-college\n",
      "10th\n",
      "Prof-school\n",
      "7th-8th\n",
      "Bachelors\n",
      "Masters\n",
      "5th-6th\n",
      "Assoc-voc\n",
      "9th\n",
      "Doctorate\n",
      "12th\n",
      "1st-4th\n",
      "Preschool\n",
      "marital-status\n",
      "Never-married\n",
      "Married-civ-spouse\n",
      "Widowed\n",
      "Separated\n",
      "Divorced\n",
      "Married-spouse-absent\n",
      "Married-AF-spouse\n",
      "occupation\n",
      "Machine-op-inspct\n",
      "Farming-fishing\n",
      "Protective-serv\n",
      "Other-service\n",
      "Prof-specialty\n",
      "Craft-repair\n",
      "Adm-clerical\n",
      "Exec-managerial\n",
      "Tech-support\n",
      "Sales\n",
      "Priv-house-serv\n",
      "Transport-moving\n",
      "Handlers-cleaners\n",
      "Armed-Forces\n",
      "relationship\n",
      "Own-child\n",
      "Husband\n",
      "Not-in-family\n",
      "Unmarried\n",
      "Wife\n",
      "Other-relative\n",
      "race\n",
      "Black\n",
      "White\n",
      "Other\n",
      "Amer-Indian-Eskimo\n",
      "Asian-Pac-Islander\n",
      "sex\n",
      "Male\n",
      "Female\n",
      "native-country\n",
      "United-States\n",
      "Peru\n",
      "Guatemala\n",
      "Mexico\n",
      "Dominican-Republic\n",
      "Ireland\n",
      "Germany\n",
      "Philippines\n",
      "Thailand\n",
      "Haiti\n",
      "El-Salvador\n",
      "Puerto-Rico\n",
      "Vietnam\n",
      "South\n",
      "Columbia\n",
      "Japan\n",
      "India\n",
      "Cambodia\n",
      "Poland\n",
      "Laos\n",
      "England\n",
      "Cuba\n",
      "Taiwan\n",
      "Italy\n",
      "Canada\n",
      "Portugal\n",
      "China\n",
      "Nicaragua\n",
      "Honduras\n",
      "Iran\n",
      "Scotland\n",
      "Jamaica\n",
      "Ecuador\n",
      "Yugoslavia\n",
      "Hungary\n",
      "Hong\n",
      "Greece\n",
      "Trinadad&Tobago\n",
      "Outlying-US\n",
      "France\n",
      "Holand-Netherlands\n"
     ]
    }
   ],
   "source": [
    "\n",
    "renglones=df['race'].shape[0]\n",
    "columnas=df_categorical.shape[1]\n",
    "one_hot=np.zeros(renglones)\n",
    "\n",
    "col=0\n",
    "df2=pd.DataFrame()\n",
    "df3=pd.DataFrame()\n",
    "df3.insert(0,'1',one_hot)\n",
    "for k in range(0,columnas):\n",
    "    name=df_categorical.columns[k]\n",
    "    print(name)\n",
    "    lista=[]\n",
    "    lista=df_categorical[name].values.tolist()\n",
    "    clases=df_categorical[name].unique() #un array de las distintas clases\n",
    "    size_clases=len(clases)\n",
    "    \n",
    "    \n",
    "    for i in range(size_clases):\n",
    "        clase=clases[i]\n",
    "        print(clase)\n",
    "\n",
    "        for j in range(0,renglones):\n",
    "            if lista[j]==clase:\n",
    "                one_hot[j]=1  \n",
    "        \n",
    "        df2.insert(i,clase,one_hot)\n",
    "        \n",
    "        one_hot=np.zeros(renglones)\n",
    "    df3=df3.join(df2)\n",
    "    df2=pd.DataFrame()\n",
    "\n",
    "\n",
    "df3.to_csv('one_hot.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y finalmente concatenamos nuestros dataframes y los guardamos en un CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "este es el dataframe numerico             age  education-num  capital-gain  capital-loss  hours-per-week\n",
      "0      0.277778         0.4375      0.000000           0.0        0.404040\n",
      "1      0.422222         0.5625      0.000000           0.0        0.505051\n",
      "2      0.311111         0.7500      0.000000           0.0        0.404040\n",
      "3      0.488889         0.6250      0.076881           0.0        0.404040\n",
      "4      0.377778         0.3750      0.000000           0.0        0.303030\n",
      "...         ...            ...           ...           ...             ...\n",
      "45217  0.300000         0.7500      0.000000           0.0        0.383838\n",
      "45218  0.444444         0.5625      0.000000           0.0        0.404040\n",
      "45219  0.644444         0.5625      0.000000           0.0        0.404040\n",
      "45220  0.244444         0.5625      0.000000           0.0        0.202020\n",
      "45221  0.577778         0.5625      0.150242           0.0        0.404040\n",
      "\n",
      "[45222 rows x 5 columns]\n",
      "(45222, 105)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>age</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>1</th>\n",
       "      <th>Private</th>\n",
       "      <th>Local-gov</th>\n",
       "      <th>Self-emp-not-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>Jamaica</th>\n",
       "      <th>Ecuador</th>\n",
       "      <th>Yugoslavia</th>\n",
       "      <th>Hungary</th>\n",
       "      <th>Hong</th>\n",
       "      <th>Greece</th>\n",
       "      <th>Trinadad&amp;Tobago</th>\n",
       "      <th>Outlying-US</th>\n",
       "      <th>France</th>\n",
       "      <th>Holand-Netherlands</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.505051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.311111</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.076881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45217</th>\n",
       "      <td>1</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.7500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45218</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45219</th>\n",
       "      <td>1</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45220</th>\n",
       "      <td>1</td>\n",
       "      <td>0.244444</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.202020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45221</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.577778</td>\n",
       "      <td>0.5625</td>\n",
       "      <td>0.150242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45222 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class       age  education-num  capital-gain  capital-loss  \\\n",
       "0         1  0.277778         0.4375      0.000000           0.0   \n",
       "1         1  0.422222         0.5625      0.000000           0.0   \n",
       "2        -1  0.311111         0.7500      0.000000           0.0   \n",
       "3        -1  0.488889         0.6250      0.076881           0.0   \n",
       "4         1  0.377778         0.3750      0.000000           0.0   \n",
       "...     ...       ...            ...           ...           ...   \n",
       "45217     1  0.300000         0.7500      0.000000           0.0   \n",
       "45218    -1  0.444444         0.5625      0.000000           0.0   \n",
       "45219     1  0.644444         0.5625      0.000000           0.0   \n",
       "45220     1  0.244444         0.5625      0.000000           0.0   \n",
       "45221    -1  0.577778         0.5625      0.150242           0.0   \n",
       "\n",
       "       hours-per-week    1  Private  Local-gov  Self-emp-not-inc  ...  \\\n",
       "0            0.404040  0.0      1.0        0.0               0.0  ...   \n",
       "1            0.505051  0.0      1.0        0.0               0.0  ...   \n",
       "2            0.404040  0.0      0.0        1.0               0.0  ...   \n",
       "3            0.404040  0.0      1.0        0.0               0.0  ...   \n",
       "4            0.303030  0.0      1.0        0.0               0.0  ...   \n",
       "...               ...  ...      ...        ...               ...  ...   \n",
       "45217        0.383838  0.0      1.0        0.0               0.0  ...   \n",
       "45218        0.404040  0.0      1.0        0.0               0.0  ...   \n",
       "45219        0.404040  0.0      1.0        0.0               0.0  ...   \n",
       "45220        0.202020  0.0      1.0        0.0               0.0  ...   \n",
       "45221        0.404040  0.0      0.0        0.0               0.0  ...   \n",
       "\n",
       "       Jamaica  Ecuador  Yugoslavia  Hungary  Hong  Greece  Trinadad&Tobago  \\\n",
       "0          0.0      0.0         0.0      0.0   0.0     0.0              0.0   \n",
       "1          0.0      0.0         0.0      0.0   0.0     0.0              0.0   \n",
       "2          0.0      0.0         0.0      0.0   0.0     0.0              0.0   \n",
       "3          0.0      0.0         0.0      0.0   0.0     0.0              0.0   \n",
       "4          0.0      0.0         0.0      0.0   0.0     0.0              0.0   \n",
       "...        ...      ...         ...      ...   ...     ...              ...   \n",
       "45217      0.0      0.0         0.0      0.0   0.0     0.0              0.0   \n",
       "45218      0.0      0.0         0.0      0.0   0.0     0.0              0.0   \n",
       "45219      0.0      0.0         0.0      0.0   0.0     0.0              0.0   \n",
       "45220      0.0      0.0         0.0      0.0   0.0     0.0              0.0   \n",
       "45221      0.0      0.0         0.0      0.0   0.0     0.0              0.0   \n",
       "\n",
       "       Outlying-US  France  Holand-Netherlands  \n",
       "0              0.0     0.0                 0.0  \n",
       "1              0.0     0.0                 0.0  \n",
       "2              0.0     0.0                 0.0  \n",
       "3              0.0     0.0                 0.0  \n",
       "4              0.0     0.0                 0.0  \n",
       "...            ...     ...                 ...  \n",
       "45217          0.0     0.0                 0.0  \n",
       "45218          0.0     0.0                 0.0  \n",
       "45219          0.0     0.0                 0.0  \n",
       "45220          0.0     0.0                 0.0  \n",
       "45221          0.0     0.0                 0.0  \n",
       "\n",
       "[45222 rows x 105 columns]"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clase=df['class']\n",
    "#print(df_clase)\n",
    "#\n",
    "df_clase=df_clase.reset_index()\n",
    "df_clase=df_clase.drop(['index'],axis=1)\n",
    "#print(df_clase)\n",
    "df_numerical=df_numerical.reset_index()\n",
    "df_numerical=df_numerical.drop(['index'],axis=1)\n",
    "print('este es el dataframe numerico',df_numerical)\n",
    "df3=df3.reset_index()\n",
    "#df3=df3.drop(['index'],axis=1)\n",
    "frames=[df_clase,df_numerical,df3]\n",
    "\n",
    "df_final=pd.concat(frames,axis=1)\n",
    "df_final=df_final.drop(['index'],axis=1)\n",
    "#df_final=df_final.drop(['class'],axis=1)\n",
    "#df_final=df_final.drop(['1'],axis=1)\n",
    "print(df_final.shape)   \n",
    "\n",
    "df_final.to_csv('final.csv')\n",
    "df_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a hacer las funciones necesarias para realizar el descenso del gradiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Model():\n",
    "    \"\"\"\n",
    "    Basic Linear Regression with Ridge Regression\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int) -> None:\n",
    "        self.dim = dim\n",
    "        self.key = random.PRNGKey(0)\n",
    "        self.cpus = jax.devices(\"cpu\")\n",
    "\n",
    "    @staticmethod\n",
    "    @jit\n",
    "    def linear_model(X: jnp, theta: jnp) -> jnp:\n",
    "        \"\"\"\n",
    "        Classic Linear Model. Jit has been used to accelerate the loops after the first one\n",
    "        for the Gradient Descent part\n",
    "        args:\n",
    "            X: Data array at the GPU or CPU\n",
    "            theta: Parameter w for weights and b for bias\n",
    "        returns:\n",
    "            f(x): the escalar estimation on vector x or the array of estimations\n",
    "        \"\"\"\n",
    "        w = theta[:-1]\n",
    "        b = theta[-1]\n",
    "        return jax.numpy.matmul(X, w) + b\n",
    "\n",
    "    def generate_theta(self):\n",
    "        \"\"\"\n",
    "        Use the random generator at Jax to generate a random generator to instanciate\n",
    "        the augmented values\n",
    "        \"\"\"\n",
    "        keys = random.split(self.key, 1)\n",
    "        return jax.numpy.vstack([random.normal(keys[0], (self.dim,1)), jax.numpy.array(0)])\n",
    "        \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def LSE(self, theta: jnp, X: jnp, y: jnp)-> jnp:\n",
    "        \"\"\"\n",
    "        LSE in matrix form. We also use Jit por froze info at self to follow \n",
    "        the idea of functional programming on Jit for no side effects\n",
    "        args:\n",
    "            X: Data array at the GPU or CPU\n",
    "            theta: Parameter w for weights and b for bias\n",
    "            y: array of labels\n",
    "        returns:\n",
    "            the Loss function LSE under data X, labels y and theta initial estimation\n",
    "        \"\"\"\n",
    "        return (jax.numpy.transpose(y - self.linear_model(X, theta))@(y - self.linear_model(X, theta)))[0,0]\n",
    "\n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def update(self, theta: jnp, X: jnp, y: jnp, lr):\n",
    "        \"\"\"\n",
    "        Update makes use of the autograd at Jax to calculate the gradient descent.\n",
    "        args:\n",
    "            X: Data array at the GPU or CPU\n",
    "            theta: Parameter w for weights and b for bias\n",
    "            y: array of labels\n",
    "            lr: Learning rate for Gradient Descent\n",
    "        returns:\n",
    "            the step update w(n+1) = w(n)-δ(t)𝜵L(w(n))        \n",
    "        \"\"\"\n",
    "        return theta - lr * jax.grad(self.LSE)(theta, X, y)\n",
    "\n",
    "        \n",
    "    @partial(jit, static_argnums=(0,))\n",
    "    def estimate_grsl(self, X, theta):\n",
    "        \"\"\"\n",
    "        Estimation for the Gradient Descent version\n",
    "        args:\n",
    "            X: Data array at the GPU or CPU\n",
    "            theta: Parameter w for weights and b for bias\n",
    "        return:\n",
    "            Estimation of data X under linear model\n",
    "        \"\"\"\n",
    "        w = theta[:-1]\n",
    "        b = theta[-1]\n",
    "        return X@w+b\n",
    "    \n",
    "    def precision(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Precision\n",
    "        args:\n",
    "            y: Real Labels\n",
    "            y_hat: estimated labels\n",
    "        return TP/(TP+FP)\n",
    "        \"\"\"\n",
    "        TP=0\n",
    "        FP=0\n",
    "        for i in range(len(y)):\n",
    "            if(y_hat[i]>0 and y[i]>0):\n",
    "                TP+=1\n",
    "            if(y_hat[i]>0 and y[i]<0):\n",
    "                FP+=1\n",
    "\n",
    "        #TP = sum(y_hat[y>0]>0)\n",
    "        #FP = sum(y_hat[y>0]<0)\n",
    "        precision_cpu = jax.jit(lambda x: x, device=self.cpus[0])(TP/(TP+FP))\n",
    "        return float(precision_cpu)\n",
    "    def accuracy(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Precision\n",
    "        args:\n",
    "            y: Real Labels\n",
    "            y_hat: estimated labels\n",
    "        return TP/(TP+FP)\n",
    "        \"\"\"\n",
    "        TP=0\n",
    "        TN=0\n",
    "        FP=0\n",
    "        FN=0\n",
    "        for i in range(len(y)):\n",
    "            if(y_hat[i]>0 and y[i]>0) :\n",
    "                TP+=1\n",
    "            if(y_hat[i]<0 and y[i]<0):\n",
    "                TN+=1\n",
    "            if(y_hat[i]<0 and y[i]>0):\n",
    "                FN+=1\n",
    "            if(y_hat[i]>0 and y[i]<0):\n",
    "                FP+=1\n",
    "\n",
    "        #TP = sum(y_hat[y>0]>0)\n",
    "        #FP = sum(y_hat[y>0]<0)\n",
    "        accuracy_cpu = jax.jit(lambda x: x, device=self.cpus[0])((TP+TN)/(TP+FP+TN+FN))\n",
    "        return float(accuracy_cpu)\n",
    "    \n",
    "    \n",
    "    def gradient_descent(self, theta: jnp,  X: jnp, y: jnp, n_steps: int, lr = 0.001):\n",
    "        \"\"\"\n",
    "        Gradient Descent Loop for the LSE Linear Model\n",
    "        args:\n",
    "            X: Data array at the GPU or CPU\n",
    "            theta: Parameter w for weights and b for bias\n",
    "            y: array of labels\n",
    "            n_steps: number steps for the Gradient Loop\n",
    "            lr: Learning rate for Gradient Descent   \n",
    "        return:\n",
    "            Updated Theta\n",
    "        \"\"\"\n",
    "        for i in range(n_steps):\n",
    "            theta = self.update(theta, X, y, lr)\n",
    "        return theta\n",
    "    ######################################################################################################\n",
    "    #########vamos a hacer la implementación de la regularización de Ridge##################################\n",
    "\n",
    "    def generate_canonicalRidge_estimator(self, X: jnp, y:jnp,la:jnp) -> jnp:\n",
    "        \"\"\"\n",
    "        Cannonical LSE error solution for the Linearly separable classes \n",
    "        args:\n",
    "            X: Data array at the GPU or CPU\n",
    "            y: Label array at the GPU \n",
    "        returns:\n",
    "            w: Weight array at the GPU or CPU\n",
    "        \"\"\"\n",
    "        XX=jax.numpy.transpose(X)@X\n",
    "        dimension=int(jnp.shape(XX)[0])\n",
    "        I=jax.numpy.identity(dimension)\n",
    "        return  jax.numpy.linalg.inv(XX+la*I)@jax.numpy.transpose(X)@y\n",
    "    \n",
    "    @staticmethod\n",
    "    def estimate_cannonicalRidge(X: jnp, w: jnp)->jnp:\n",
    "        \"\"\"\n",
    "        Estimation for the Gradient Descent version\n",
    "        args:\n",
    "            X: Data array at the GPU or CPU\n",
    "            w: Parameter w under extended space\n",
    "        return:\n",
    "            Estimation of data X under cannonical solution\n",
    "        \"\"\"\n",
    "        return X@w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a cargar nuestros datos limpiados en jnp arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45222, 1)"
      ]
     },
     "execution_count": 590,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X=df_final[df_final.columns[1:]].to_numpy()\n",
    "y=df_clase.to_numpy()\n",
    "y=y.astype(int)\n",
    "dim=int(X.shape[1])\n",
    "print(dim)\n",
    "X=jax.numpy.asarray(X)\n",
    "y=jax.numpy.asarray(y)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105, 1)"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Linear_Model(dim)\n",
    "theta = model.generate_theta()\n",
    "theta = model.gradient_descent(theta, X, y, 10000, lr = 0.0000001)\n",
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 104)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7960000038146973"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_est=y[:500]\n",
    "X_est=X[:500]\n",
    "print(X_est.shape)\n",
    "y_hat=model.estimate_grsl(X_est,theta)\n",
    "recall=model.precision(y_est,y_hat)\n",
    "accuracy=model.accuracy(y_est, y_hat)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6710000038146973"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último vamos a aplicar la version regularizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primero necesitamos la X aumentada\n",
    "X_e = np.hstack([X_est, np.ones((500,1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8740919828414917 0.01\n",
      "0.8740919828414917 0.02\n",
      "0.8740919828414917 0.03\n",
      "0.8740919828414917 0.04\n",
      "0.8740919828414917 0.05\n",
      "0.8740919828414917 0.060000000000000005\n",
      "0.8737863898277283 0.07\n",
      "0.8737863898277283 0.08\n",
      "0.8737863898277283 0.09\n",
      "0.8737863898277283 0.09999999999999999\n",
      "0.8737863898277283 0.10999999999999999\n",
      "0.8737863898277283 0.11999999999999998\n",
      "0.8716707229614258 0.12999999999999998\n",
      "0.8716707229614258 0.13999999999999999\n",
      "0.8716707229614258 0.15\n",
      "0.8716707229614258 0.16\n",
      "0.8716707229614258 0.17\n",
      "0.8716707229614258 0.18000000000000002\n",
      "0.8716707229614258 0.19000000000000003\n",
      "0.8716707229614258 0.20000000000000004\n",
      "0.8716707229614258 0.21000000000000005\n",
      "0.8716707229614258 0.22000000000000006\n",
      "0.8716707229614258 0.23000000000000007\n",
      "0.8716707229614258 0.24000000000000007\n",
      "0.8716707229614258 0.25000000000000006\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#vamos a buscar lambda en un grid de puntos[0,0.5,1,...]\n",
    "l=0\n",
    "max_steps=25\n",
    "l_vec=np.zeros(int(max_steps))\n",
    "precision=np.zeros(int(max_steps))\n",
    "for i in range(max_steps):\n",
    "    l=l+0.01\n",
    "    l_vec[i]=l\n",
    "    wR = model.generate_canonicalRidge_estimator(X_e, y_est,l)\n",
    "    y_hatR = model.estimate_cannonicalRidge(X_e, wR)\n",
    "    precision[i]=model.precision(y_est, y_hatR)\n",
    "    print(precision[i],l)\n",
    "\n",
    "plt.plot(l_vec,precision,'*-')\n",
    "plt.xlabel('lambda')\n",
    "plt.ylabel('recall')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "204b973de28be2f053450059895fb876f6b1718ca01aa708473ad2968cb937fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
